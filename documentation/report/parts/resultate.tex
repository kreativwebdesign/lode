%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../doc"
%%% coding: utf-8
%%% End:
% !TEX TS-program = pdflatexmk
% !TEX encoding = UTF-8 Unicode
% !TEX root = ../doc.tex
Das Resultat besteht wie beschrieben aus verschiedenen Artefakten. In diesem Abschnitt wird konkret auf die Resultate der verschiedenen Schritte eingegangen und der Zusammenhang hergestellt.

\section{LOD Pipeline}

Der Kern der Arbeit stellt die LOD Pipeline, \e{lode} genannt, dar. Dieses Tool liefert eine für das Web optimierte Möglichkeit LOD Artefakte für eine Vielzahl von Anwendungsgebieten zu generieren.

\subsection{Workflow}

Der übliche Ablauf für das generieren von LOD Artefakten ist wie folgt:
Für ein gegebenes Modell werden innerhalb vom Modellierungstool wie z.B. Blender bestimmte LOD Stufen von Hand generiert. Anschliessend werden die verschiedenen Stufen exportiert und manuell in die Applikation integriert. Fine Tuning erfordert so sowohl Anpassungen an den Modellen als auch im Code.

\subsection{lode Pipeline}

Das Ziel von \e{lode} ist es, für ein möglichst breites Spektrum von Anwendungsfällen eine einfache Lösung anzubieten und somit die Hemmschwelle für den Einsatz von LOD Artfakten zu reduzieren.

Deshalb setzt \e{lode} konsequent auf moderne Entwicklungsprozesse, manuelle Schritte sollen auf ein Minimum reduziert werden.

\subsection{lode Ablauf}

Ein Modell wird erstellt und als glTF innerhalb des Projekts gespeichert. Eine zum Modell gehörige Konfigurationsdatei wird angelegt und \e{lode} generiert die darin beschriebenen LOD Artfakte. Die Artfakten können innerhalb der Applikation mithilfe einer Bibliothek geladen werden. Bei Anpassungen an der Konfigurationsdatei werden die neuen Detailstufen automatisch generiert. Das zugehörige \e{lode-ui} bietet zudem die Möglichkeit die verschiedenen Detailstufen miteinander vergleichen zu können und die Konfiguration möglichst einfach zu justieren.

\section{Benchmark}

In diesem Abschnitt werden die verschiedenen erwägten Optionen erläutert und die Abgrenzungen aufgezeigt.
Das Ziel des Benchmarks ist der Vergleich einer Version ohne Einsatz von LOD und einer Umgebung, welche LODs einsetzt. Um eine möglichst praxisnahe Aussage treffen zu können wird hierfür eine Demo Szenerie verwendet, welche einer echten Anwendung so nah wie möglich kommt. Somit wird gewährleistet, dass es nicht nur in der Theorie einen Nutzen für LODs gibt, sondern dieser in der Praxis vorhanden ist.

\paragraph{Browser Umgebung}
Um den Umfang des Benchmarks überschaubar zu halten, wurde ausschliesslich ein Benchmark für Google Chrome entwickelt.
Google Chrome basiert auf \fgls{Chromium}{Open Source Browser-Projekt welches von Google entwickelt wird.}, dieselbe Engine, welche auch Microsoft Edge oder Opera verwenden.
Einen Benchmark basierend auf Google Chrome liefert somit auch Indizien für diese beiden Browser, auch wenn gewisse Abweichungen möglich sind.
Neben dem Marktführer Chrome sind Mozilla Firefox oder Safari von Apple ebenfalls Optionen. Jedoch wurde primär aufgrund des Marktanteils von total rund 70\% \cite{browserUsage} zugunsten von Google Chrome entschieden.
Die getroffenen Aussagen bezüglich Laufzeitverhalten behalten ihre Gültigkeit auch für andere Browser.

\paragraph{Automation}
Um die Tests durchzuführen, wird ein Testautomationstool benötigt; unter anderem der Einsatz von Selenium wurde in Erwägung gezogen.
Der Vorteil von Selenium ist insbesondere, dass der Benchmark für weitere Browser ausgeweitet werden könnte.
Da jedoch das Analysieren der GPU Daten stark vom System abhängig ist und dafür zusätzliche Komplexität notwendig wäre, wird in diesem Benchmark die im Google Chrome integrierten \e{Chrome DevTools} eingesetzt.
Selenium bietet zurzeit eine suboptimale Integration für das \e{Chrome DevTools Protocol}.
Um mögliche Diskrepanzen zwischen Systemen möglichst gering zu halten wurde jedoch entschieden auf die bewährte Lösung von Google Chrome zu setzen.
\fgls{Puppeteer}{Node.js Library, die eine API anbietet zum Steuern von \gls{Chromium} oder Chrome über das Chrome DevTools Protocol}, eine weitere Option für die Automation, ist eine Bibliothek, die eine vereinfachte Schnittstelle zu einer Headless Chrome Instanz bietet.
Sie wird zudem direkt von Google entwickelt und bietet somit eine stabile Grundlage zur Kommunikation mit den \e{Chrome DevTools}.

\paragraph{Profiling}
Die \e{Chrome DevTools} erlauben es, ein detailliertes Laufzeitprofil einer Applikation anzulegen.
Im Profil befinden sich Informationen zu CPU- und GPU-Auslastung aber auch generelle Informationen bzgl. der \gls{Rendering Engine} werden gesammelt.
Die Analyse dieser Daten ermöglicht es, eine Aussage zum Laufzeitverhalten einer Applikation zu tätigen.

\paragraph{Testaufbau}
Derselbe Testablauf wird sowohl für die optimierte als auch für die unoptimierte Version verwendet.
Bei einem Testablauf werden folgende Schritte durchlaufen:

\begin{enumerate}
  \item Öffne Applikation in \emph{Headless Chrome} Instanz.
  \item Warte bis Seite geladen ist.
  \item Starte \emph{Profiling}.
  \item Warte $n$ Sekunden.
  \item Stoppe \emph{Profiling}.
  \item Werte Daten aus.
\end{enumerate}

Der Test erfasst die in Tabelle \ref{table:benchmarkFigures} aufgeführten Kennzahlen.

\begin{table}[H]
  \centering
  \begin{tabular}{ l p{8cm} }
  \hline
  Kennzahl & Beschreibung \\
  \hline
  \hline
  Median \e{\gls{FPS}} & Die \e{\gls{FPS}} werden kontinuierlich berechnet. Um krasse Abweichungen zu ignorieren wird der Median verwendet. \\
  \hline
  Totale GPU Zeit & Die totale Zeit, welche die GPU für Berechungen benötigt. \\
  \hline
  Median Render Loop Dauer & Der Median aller Laufzeiten der Render Loop. \\
  \hline
  Anzahl GPU Events & Anzahl der Events an die GPU. \\
  \hline
  Zeit für das Laden der Modelle & Totale Zeit für das Laden aller Modelle der Szenerie. \\
  \hline
  Anzahl Renders & Wie oft wurde ein neues Bild gezeichnet. \\
  \hline
  \end{tabular}
  \caption{Kennzahlen für Benchmark}
  \label{table:benchmarkFigures}
\end{table}

\paragraph{Aufbau Testapplikation}
Die Testapplikation stellt eine komplexe Szenerie dar. Der Betrachter fliegt während dem Ablauf kontinuierlich über die Szenerie. Dies stellt die optimalen Bedingungen für den Einsatz von LOD Artefakten dar.

\paragraph{Testumgebung}
Um während den Tests möglichst faire Bedingungen zu gewährleisten wird die Maschine zuvor wie bei anderen Benchmarks vorbereitet. Ziel ist es, Seiteneffekte zu minimieren. Für diesen Benchmark wurden deshalb die Instruktionen von \e{Tracer Bench} zur Behebung von Rauschen befolgt.
\cite{tracerBenchNoiseMitigation}

\paragraph{Analyse der Daten}
Um eine zuverlässige Aussage treffen zu können, wird der Vorgang mehrfach wiederholt. Für jeden Durchlauf wird der Median der \e{\gls{FPS}} Daten berechnet.
Anschliessend wird die Varianz der \e{\gls{FPS}} für die unoptimierten respektive optimierten Werte berechnet. Die Varianz dient als Kennzahl, um eine Signifikanz der Daten nachweisen zu können.

\subparagraph{Konfidenzintervall}
Die Signifikanz wird mithilfe eines statistischen Konfidenzintervalls nachgewiesen. Hierfür wird der Durchschnitt aller Mediane verwendet, zusätzlich wird ein Konfidenzintervall von 95\% gewählt. Somit wird gewährleistet, dass das Resultat des Benchmarks verlässlich ist und für einen Vergleich verwendet werden kann.
